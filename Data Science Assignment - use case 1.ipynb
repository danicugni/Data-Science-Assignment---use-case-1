{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"left\"> Use case: Early expired loans </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this assignment is to evaluate the candidate's technical skills and their approach to a Data Science problem. Specifically, the task is to develop a predictive model to classify the probability that a customer will pay back and close the loan before the contract end date.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Inside the **data** folder, you will find the use_case_customer_data.csv and use_case_loans_data.csv files, which contains all the variables described below. The target variable \"STATUS\" indicates whether the customer has closed the loan regularly or early.\n",
    "\n",
    "**Assignment:**\n",
    "\n",
    "The request of this assignment is to build a predictive model with satisfactory performance, demonstrating all the typical steps that should be addressed in a Data Science project: from data cleaning and preparation to testing the performance of the constructed model.\n",
    "\n",
    "The completed notebook should be properly commented and should be delivered through sharing a personal accessible GitHub repository that allows for its reproduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Datasets details\n",
    "\n",
    "### *use_case_customer_data.csv*\n",
    "\n",
    "Variables:\n",
    "- **CUSTOMER_ID**: customer ID\n",
    "\n",
    "- **SEX**: gender of the customer\n",
    "    - M: MAN\n",
    "    - W: WOMAN\n",
    "    \n",
    "- **AGE**: age of the customer\n",
    "\n",
    "- **ANNUAL_INCOME**: annual salary value of the customer\n",
    "\n",
    "- **NUMBER_OF_MONTHS**: monthly salary number\n",
    "\n",
    "- **MARITAL_STATUS**: marital status of the customer\n",
    "    - D: DIVORCED\n",
    "    - G: SINGLE\n",
    "    - C: COHABITANT\n",
    "    - J: CONJUGATE\n",
    "    - S: SEPARATE\n",
    "    - W: WIDOWER\n",
    "    - X: OTHER\n",
    "\n",
    "- **LEASE**: type of customer lease\n",
    "    - P: PROPERTY\n",
    "    - E: AT THE EMPLOYER\n",
    "    - R: RENT\n",
    "    - A: PARENTS/RELATIVES\n",
    "    - T: THIRD PARTIES\n",
    "    - X: OTHER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *use_case_loans_data.csv*\n",
    "\n",
    "Variables:\n",
    "- **CUSTOMER_ID**: customer ID\n",
    "\n",
    "- **STATUS**: loan status (target)\n",
    "    - CONCLUDED REGULARLY\n",
    "    - EARLY EXPIRED\n",
    "\n",
    "- **SECTOR_TYPE**: type of loan\n",
    "    - CL: CAR LOAN\n",
    "    - FL: FINALIZED LOAN\n",
    "    - PL: PERSONAL LOAN\n",
    "\n",
    "- **GOOD_VALUE**: value of the mortgaged property\n",
    "\n",
    "- **ADVANCE_VALUE**: advance paid\n",
    "\n",
    "- **LOAN_VALUE**: value of the loan\n",
    "\n",
    "- **INSTALLMENT_VALUE**: value of the installment\n",
    "\n",
    "- **NUMBER_INSTALLMENT**: number of installments\n",
    "\n",
    "- **GAPR**: Gross Annual Percentage Rate\n",
    "\n",
    "- **NIR**: Nominal Interest Rate \n",
    "\n",
    "- **REFINANCED**: loan subject to refinancing (Y / N)\n",
    "\n",
    "- **FROM_REFINANCE**: loan from a refinancing (Y / N)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.0 or less. Got NumPy 2.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\shap\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explanation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\shap\\_explanation.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mslicer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpChain\n\u001b[0;32m     16\u001b[0m op_chain_root \u001b[38;5;241m=\u001b[39m OpChain(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshap.Explanation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\shap\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_clustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     delta_minimization_order,\n\u001b[0;32m      3\u001b[0m     hclust,\n\u001b[0;32m      4\u001b[0m     hclust_ordering,\n\u001b[0;32m      5\u001b[0m     partition_tree,\n\u001b[0;32m      6\u001b[0m     partition_tree_shuffle,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     OpChain,\n\u001b[0;32m     10\u001b[0m     approximate_interactions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     suppress_stderr,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_masked_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedModel, make_masks\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\shap\\utils\\_clustering.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_progress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_progress\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartition_tree\u001b[39m(X, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\numba\\__init__.py:59\u001b[0m\n\u001b[0;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32mc:\\Users\\dcugnigni\\Desktop\\Area_personale\\Fastweb\\Data Science Assignment _ use case 1\\.venv\\lib\\site-packages\\numba\\__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.0 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.1."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn import metrics\n",
    "# from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DF_CDATA = Path(r\".\\data\\use_case_customer_data.csv\")\n",
    "PATH_DF_LDATA = Path(r\".\\data\\use_case_loans_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdata = pd.read_csv(PATH_DF_CDATA)\n",
    "df_ldata = pd.read_csv(PATH_DF_LDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ANNUAL_INCOME</th>\n",
       "      <th>NUMBER_OF_MONTHS</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>LEASE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1088</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>25200.00</td>\n",
       "      <td>14</td>\n",
       "      <td>J</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1097</td>\n",
       "      <td>W</td>\n",
       "      <td>45</td>\n",
       "      <td>26610.78</td>\n",
       "      <td>14</td>\n",
       "      <td>J</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102</td>\n",
       "      <td>M</td>\n",
       "      <td>49</td>\n",
       "      <td>24700.00</td>\n",
       "      <td>13</td>\n",
       "      <td>J</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1104</td>\n",
       "      <td>W</td>\n",
       "      <td>45</td>\n",
       "      <td>15951.00</td>\n",
       "      <td>13</td>\n",
       "      <td>J</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1106</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>28114.45</td>\n",
       "      <td>13</td>\n",
       "      <td>J</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID SEX  AGE  ANNUAL_INCOME  NUMBER_OF_MONTHS MARITAL_STATUS LEASE\n",
       "0         1088   M   36       25200.00                14              J     P\n",
       "1         1097   W   45       26610.78                14              J     P\n",
       "2         1102   M   49       24700.00                13              J     P\n",
       "3         1104   W   45       15951.00                13              J     P\n",
       "4         1106   M   47       28114.45                13              J     P"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>SECTOR_TYPE</th>\n",
       "      <th>GOOD_VALUE</th>\n",
       "      <th>ADVANCE_VALUE</th>\n",
       "      <th>LOAN_VALUE</th>\n",
       "      <th>INSTALLMENT_VALUE</th>\n",
       "      <th>NUMBER_INSTALLMENT</th>\n",
       "      <th>GAPR</th>\n",
       "      <th>NIR</th>\n",
       "      <th>REFINANCED</th>\n",
       "      <th>FROM_REFINANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1088</td>\n",
       "      <td>CONCLUDED REGULARLY</td>\n",
       "      <td>FL</td>\n",
       "      <td>469.00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>44.45</td>\n",
       "      <td>10</td>\n",
       "      <td>21.74882</td>\n",
       "      <td>19.84123</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1088</td>\n",
       "      <td>CONCLUDED REGULARLY</td>\n",
       "      <td>FL</td>\n",
       "      <td>794.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>794.30</td>\n",
       "      <td>70.00</td>\n",
       "      <td>12</td>\n",
       "      <td>9.42200</td>\n",
       "      <td>9.03804</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>CONCLUDED REGULARLY</td>\n",
       "      <td>FL</td>\n",
       "      <td>399.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.00</td>\n",
       "      <td>69.85</td>\n",
       "      <td>6</td>\n",
       "      <td>8.19200</td>\n",
       "      <td>0.03800</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>CONCLUDED REGULARLY</td>\n",
       "      <td>FL</td>\n",
       "      <td>1039.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1039.98</td>\n",
       "      <td>52.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>EARLY EXPIRED</td>\n",
       "      <td>CL</td>\n",
       "      <td>23500.00</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>21387.86</td>\n",
       "      <td>310.00</td>\n",
       "      <td>84</td>\n",
       "      <td>6.72602</td>\n",
       "      <td>5.76090</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID               STATUS SECTOR_TYPE  GOOD_VALUE  ADVANCE_VALUE  \\\n",
       "0         1088  CONCLUDED REGULARLY          FL      469.00           69.0   \n",
       "1         1088  CONCLUDED REGULARLY          FL      794.30            0.0   \n",
       "2         1097  CONCLUDED REGULARLY          FL      399.00            0.0   \n",
       "3         1097  CONCLUDED REGULARLY          FL     1039.98            0.0   \n",
       "4         1097        EARLY EXPIRED          CL    23500.00         3500.0   \n",
       "\n",
       "   LOAN_VALUE  INSTALLMENT_VALUE  NUMBER_INSTALLMENT      GAPR       NIR  \\\n",
       "0      400.00              44.45                  10  21.74882  19.84123   \n",
       "1      794.30              70.00                  12   9.42200   9.03804   \n",
       "2      419.00              69.85                   6   8.19200   0.03800   \n",
       "3     1039.98              52.00                  20   0.00220   0.00220   \n",
       "4    21387.86             310.00                  84   6.72602   5.76090   \n",
       "\n",
       "  REFINANCED FROM_REFINANCE  \n",
       "0          N              N  \n",
       "1          N              N  \n",
       "2          N              N  \n",
       "3          N              N  \n",
       "4          Y              N  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Customer data: (9561, 7)\n",
      "Shape of Loans data: (37291, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Customer data:\", df_cdata.shape)\n",
    "print(\"Shape of Loans data:\", df_ldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of Customer data: ['CUSTOMER_ID', 'SEX', 'AGE', 'ANNUAL_INCOME', 'NUMBER_OF_MONTHS', 'MARITAL_STATUS', 'LEASE']\n",
      "Columns of Loans data: ['CUSTOMER_ID', 'STATUS', 'SECTOR_TYPE', 'GOOD_VALUE', 'ADVANCE_VALUE', 'LOAN_VALUE', 'INSTALLMENT_VALUE', 'NUMBER_INSTALLMENT', 'GAPR', 'NIR', 'REFINANCED', 'FROM_REFINANCE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns of Customer data:\", df_cdata.columns.to_list())\n",
    "print(\"Columns of Loans data:\", df_ldata.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'STATUS'\n",
    "FEATURES = [c for c in df_ldata.columns if c != TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9561"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cdata['CUSTOMER_ID'].nunique() #ciacun ID è presente una sola volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9561"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ldata['CUSTOMER_ID'].nunique() #ci sono ID che si ripetono, ma il numero di ID unici è uguale al numero di ID unici in df_cdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si effettua lo split in training e test solamente di *df_ldata*, poichè *df_cdata* contiene informazioni per ciascun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_ldata, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set: (29832, 12)\n",
      "Shape of test set: (7459, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train set:\", df_train.shape)\n",
    "print(\"Shape of test set:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[FEATURES]\n",
    "X_test = df_test[FEATURES]\n",
    "y_train = df_train[TARGET]\n",
    "y_test = df_test[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUSTOMER_ID          0\n",
       "SEX                  0\n",
       "AGE                  0\n",
       "ANNUAL_INCOME        0\n",
       "NUMBER_OF_MONTHS     0\n",
       "MARITAL_STATUS      15\n",
       "LEASE                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cdata.isna().sum() # MARITAL_STATUS e LEASE sono le uniche variabili che contengono NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUSTOMER_ID           0\n",
       "STATUS                0\n",
       "SECTOR_TYPE           0\n",
       "GOOD_VALUE            0\n",
       "ADVANCE_VALUE         0\n",
       "LOAN_VALUE            0\n",
       "INSTALLMENT_VALUE     0\n",
       "NUMBER_INSTALLMENT    0\n",
       "GAPR                  0\n",
       "NIR                   0\n",
       "REFINANCED            0\n",
       "FROM_REFINANCE        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ldata.isna().sum() #non ci sono valori mancanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPUTE NA VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avendo valori NA di variabili categoriali, si decide di inserire la modalità \"Not available\" al posto dei valori mancanti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARITAL_STATUS', 'LEASE']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_with_na = df_cdata.columns[df_cdata.isna().sum() > 0].tolist()\n",
    "vars_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na(df, var):\n",
    "    df_copy = df.copy()\n",
    "    mask_na = df_copy[var].isna()\n",
    "    df_copy.loc[mask_na, var] = 'Not available'\n",
    "    # df_copy['IS_NA_' + var] = mask_na\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars_with_na:\n",
    "    df_cdata = impute_na(df_cdata, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUSTOMER_ID         0\n",
       "SEX                 0\n",
       "AGE                 0\n",
       "ANNUAL_INCOME       0\n",
       "NUMBER_OF_MONTHS    0\n",
       "MARITAL_STATUS      0\n",
       "LEASE               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cdata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS\n",
       "CONCLUDED REGULARLY    30037\n",
       "EARLY EXPIRED           7254\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ldata['STATUS'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS\n",
       "CONCLUDED REGULARLY    0.805476\n",
       "EARLY EXPIRED          0.194524\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ldata['STATUS'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS - HYPERPARAMETERS TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_selection(model:str = \"logistic\", out_folder:str = \".\\models\",  n_fold:int = 5, n_iter:int = 15):\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "    date_output = today.strftime(\"%d_%m_%Y\")\n",
    "    \n",
    "    if model == \"logistic\":\n",
    "        params = {\n",
    "            \"penalty\": [None, \"l1\", \"l2\"],\n",
    "            \"C\": [1e-2, 1e-1, 1, 2, 5, 10],\n",
    "            \"max_iter\": [100, 200, 300],\n",
    "            \"random_state\": [1],\n",
    "            \"class_weight\":[None, \"balanced\"],\n",
    "            \"solver\": [\"saga\"]\n",
    "        }\n",
    "\n",
    "        # gs = RandomizedSearchCV(LogisticRegression(), params, cv = n_fold, n_iter=n_iter)\n",
    "        gs = GridSearchCV(LogisticRegression(), params, cv = n_fold)\n",
    "        \n",
    "    elif model == \"tree\":\n",
    "        #Decision Tree classifier\n",
    "        params = {\n",
    "            \"max_depth\": [None, 10, 20, 40, 50],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 5],\n",
    "            \"class_weight\":[None, \"balanced\"],\n",
    "            \"random_state\": [1]}\n",
    "\n",
    "        # gs = RandomizedSearchCV(DecisionTreeClassifier(), params, cv = n_fold, n_iter=n_iter n_jobs = -1)\n",
    "        gs = GridSearchCV(DecisionTreeClassifier(), params, cv = n_fold)\n",
    "    \n",
    "    elif model == \"random_forest\":\n",
    "        params = {\n",
    "            \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 5],\n",
    "            \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "            \"random_state\": [1],\n",
    "            \"class_weight\":[None, \"balanced\"]\n",
    "            }\n",
    "\n",
    "        # gs = RandomizedSearchCV(RandomForestClassifier(), params, cv = n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(RandomForestClassifier(), params, cv = n_fold)\n",
    "    \n",
    "    elif model == \"svc\":\n",
    "        params = {\n",
    "            \"C\": [1e-1, 1, 2, 5, 10],\n",
    "            \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"degree\":[2,3,4,5],\n",
    "            \"gamma\":[\"scale\", \"auto\"],\n",
    "            \"class_weight\":[None, \"balanced\"],\n",
    "            \"random_state\": [1]\n",
    "            }\n",
    "\n",
    "        # gs = RandomizedSearchCV(SVC(), params, cv = n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(SVC(), params, cv = n_fold)\n",
    "\n",
    "    elif model == \"knn\":\n",
    "        params = {\n",
    "            \"n_neighbors\": [2, 5, 10, 15, 20],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"p\":[1,2],\n",
    "            \"random_state\": [1]\n",
    "            }\n",
    "\n",
    "        # gs = RandomizedSearchCV(SVC(), params, cv =n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(KNeighborsClassifier(), params, cv = n_fold)\n",
    "        \n",
    "    elif model == \"extra_tree\":\n",
    "        params = {\n",
    "            \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 5],\n",
    "            \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "            \"random_state\": [1],\n",
    "            \"class_weight\":[None, \"balanced\"]\n",
    "            }\n",
    "\n",
    "        # gs = RandomizedSearchCV(ExtraTreesClassifier(), params, cv = n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(ExtraTreesClassifier(), params, cv = n_fold)\n",
    "    \n",
    "    elif model == \"xgb\":\n",
    "        params = {\n",
    "            \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "            \"max_depth\": [2, 5, 10, 15],\n",
    "            \"learning_rate\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"reg_alpha\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1],\n",
    "            \"reg_lambda\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1]\n",
    "        }\n",
    "\n",
    "        # gs = RandomizedSearchCV(XGBClassifier(), params, cv = n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(XGBClassifier(), params, cv = n_fold)\n",
    "    \n",
    "    elif model == \"light_gbm\":\n",
    "        params = {\n",
    "            \"bagging_fraction\": [0.2, 0.4, 0.6, 0.8, 1.0], \n",
    "            \"feature_fraction\": [0.6, 0.8, 1.0], \n",
    "            \"learning_rate\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1], \n",
    "            \"max_depth\": [2, 5, 10, 15], \n",
    "            \"n_estimators\": [100, 200, 300, 400], \n",
    "            # \"num_leaves\": [5, 10, 15, 20, 30, 35], \n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "            \"random_state\":[1],\n",
    "            # \"min_data_in_leaf\": [5, 10, 15, 20], \n",
    "            # \"max_bin\": [5, 10, 15, 20],\n",
    "            # \"subsample\": 1.0,\n",
    "            # \"min_sum_hessian_in_leaf\": 0.001, \n",
    "            }\n",
    "\n",
    "        # gs = RandomizedSearchCV(LGBMClassifier(), params, cv = n_fold, n_iter=n_iter, n_jobs=-1)\n",
    "        gs = GridSearchCV(LGBMClassifier(), params, cv = n_fold)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Modello non valido.\")\n",
    "        \n",
    "\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    best_params = gs.best_params_\n",
    "    best_model = gs.best_estimator_\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "\n",
    "    FULL_OUT = os.path.join(out_folder, date_output, model)\n",
    "    with open(FULL_OUT, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prova\n",
    "hyperparams_selection(\"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"logistic\", \"tree\", \"random_forest\", \"svc\", \"knn\", \"extra_tree\", \"xgb\", \"light_gbm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    hyperparams_selection(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS - EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "roc_auc = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test_set(model:str = \"logistic\", out_folder:str = \".\\models\"):\n",
    "\n",
    "    FULL_OUT = os.path.join(out_folder, date_output, model)\n",
    "    with open(FULL_OUT, 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "\n",
    "    models.append(model)\n",
    "    recall.append(metrics.recall_score(y_test, y_predicted))\n",
    "    precision.append(metrics.precision_score(y_test, y_predicted))\n",
    "    f1.append(metrics.f1_score(y_test, y_predicted))\n",
    "    roc_auc.append(metrics.roc_auc_score(y_test, y_predicted))\n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    eval_test_set(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"Models\": models,\n",
    "    \"F1\": f1,\n",
    "    \"Recall\": recall,\n",
    "    \"Precision\": precision,\n",
    "    \"Roc_auc\": roc_auc,\n",
    "    \"Accuracy\": accuracy\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETABILITY OF THE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO UTILS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now()\n",
    "date_output = today.strftime(\"%d_%m_%Y\")\n",
    "name_output = \"stringa\"\n",
    "\n",
    "#Logistic regression\n",
    "params = {\"penalty\": [None, \"l1\", \"l2\"],\n",
    "              \"C\": [1e-2, 1e-1, 1, 2, 5, 10],\n",
    "              \"max_iter\": [100, 200, 300],\n",
    "              \"multi_class\": [\"ovr\"],\n",
    "              \"random_state\": [1],\n",
    "              \"class_weight\":[None, \"balanced\"],\n",
    "            #   \"solver\": [\"saga\"]\n",
    "            }\n",
    "\n",
    "# gs = RandomizedSearchCV(LogisticRegression(), params, cv = 5, n_jobs = -1)\n",
    "gs = GridSearchCV(LogisticRegression(), params, cv = 5, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "#Decision Tree classifier\n",
    "params = {\"max_depth\": [None, 10, 20, 40, 50],\n",
    "              \"min_samples_split\": [2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 2, 5],\n",
    "              \"class_weight\":[None, \"balanced\"],\n",
    "              \"random_state\": [1]}\n",
    "\n",
    "# gs = RandomizedSearchCV(DecisionTreeClassifier(), params, cv = 5, n_jobs = -1)\n",
    "gs = GridSearchCV(DecisionTreeClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "params = {\"n_estimators\": [50, 100, 200, 300, 400],\n",
    "              \"max_depth\": [None, 10, 20],\n",
    "              \"min_samples_split\": [2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 2, 5],\n",
    "              \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "              \"random_state\": [1],\n",
    "              \"class_weight\":[None, \"balanced\"]\n",
    "              #,\"n_jobs\":[-1]\n",
    "              }\n",
    "\n",
    "# gs = RandomizedSearchCV(RandomForestClassifier(), params, cv = 5)\n",
    "gs = GridSearchCV(RandomForestClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#SVM\n",
    "params = {\"C\": [1e-1, 1, 2, 5, 10],\n",
    "              \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "              \"degree\":[2,3,4,5],\n",
    "              \"gamma\":[\"scale\", \"auto\"],\n",
    "              \"class_weight\":[None, \"balanced\"],\n",
    "              \"random_state\": [1]}\n",
    "\n",
    "# gs = RandomizedSearchCV(SVC(), params, cv = 5, n_iter=5)\n",
    "gs = GridSearchCV(SVC(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "#KNeighborsClassifier\n",
    "params = {\"n_neighbors\": [2, 5, 10, 15, 20],\n",
    "              \"weights\": [\"uniform\", \"distance\"],\n",
    "              \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "              \"p\":[1,2],\n",
    "              \"random_state\": [1]}\n",
    "\n",
    "# gs = RandomizedSearchCV(SVC(), params, cv = 5, n_iter=5)\n",
    "gs = GridSearchCV(KNeighborsClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#ExtraTreesClassifier\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "    \"random_state\": [1],\n",
    "    \"class_weight\":[None, \"balanced\"]\n",
    "    #,\"n_jobs\":[-1]\n",
    "    }\n",
    "\n",
    "# gs = RandomizedSearchCV(ExtraTreesClassifier(), params, cv = 5, n_iter=5)\n",
    "gs = GridSearchCV(ExtraTreesClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#MLPClassifier\n",
    "\n",
    "\n",
    "#XGBClassifier\n",
    "params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "    \"max_depth\": [2, 5, 10, 15],\n",
    "    \"learning_rate\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"reg_alpha\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1],\n",
    "    \"reg_lambda\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1]\n",
    "}\n",
    "\n",
    "# gs = RandomizedSearchCV(XGBClassifier(), params, cv = 5, n_iter=5)\n",
    "gs = GridSearchCV(XGBClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#LGBMClassifier\n",
    "params = {\n",
    "    \"bagging_fraction\": [0.2, 0.4, 0.6, 0.8, 1.0], \n",
    "    \"feature_fraction\": [0.6, 0.8, 1.0], \n",
    "    \"learning_rate\": [1e-3, 1e-2, 1e-1, 3e-1, 5e-1], \n",
    "    \"max_depth\": [2, 5, 10, 15], \n",
    "    \"n_estimators\": [100, 200, 300, 400], \n",
    "    # \"num_leaves\": [5, 10, 15, 20, 30, 35], \n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"random_state\":[1],\n",
    "    # \"min_data_in_leaf\": [5, 10, 15, 20], \n",
    "    # \"max_bin\": [5, 10, 15, 20],\n",
    "    # \"subsample\": 1.0,\n",
    "    # \"min_sum_hessian_in_leaf\": 0.001, \n",
    "    }\n",
    "\n",
    "# gs = RandomizedSearchCV(LGBMClassifier(), params, cv = 5, n_iter=5)\n",
    "gs = GridSearchCV(LGBMClassifier(), params, cv = 5, n_jobs= -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "FULL_OUT = os.path.join('./models', date_output, '_SVM_', name_output)\n",
    "with open(FULL_OUT, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(FULL_OUT, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "y_predicted = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "#VotingClassifier\n",
    "\n",
    "\n",
    "\n",
    "#StackingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
